# app/news_parser/news_collector.py
import asyncio
from typing import List

from app.logger import logger
from app.config import settings
from app.news_parser import parser_habr, parser_rbk, parser_telegram
from app.api.schemas import ParsedNewsSchema, SourceType


async def collect_news(limit_telegram: int = 50) -> List[ParsedNewsSchema]:
    """
    –°–±–æ—Ä –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å Habr, RBK –∏ Telegram.

    :param limit_telegram: —Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π Telegram –ø–∞—Ä—Å–∏—Ç—å
    :return: —Å–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ParsedNewsSchema
    """
    logger.info("üöÄ –°—Ç–∞—Ä—Ç —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    tasks = [
        parser_habr.parse_news_habr_site(),
        parser_rbk.parse_news_rbk_site(),
        parser_telegram.parse_telegram_channel(limit=limit_telegram),
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    raw_news = []
    for source_news in results:
        if isinstance(source_news, Exception):
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_news}")
            continue
        raw_news.extend(source_news)

    logger.info(f"–°–æ–±—Ä–∞–Ω–æ –≤—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π (–¥–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏): {len(raw_news)}")

    # =========================
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ Pydantic
    # =========================
    validated_news: List[ParsedNewsSchema] = []
    for item in raw_news:
        try:
            validated = ParsedNewsSchema.model_validate(item)
            validated_news.append(validated)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–æ–≤–æ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∞ (–Ω–µ –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é): {e}")

    logger.info(f"–ü–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(validated_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    # =========================
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL
    # =========================
    seen_urls = set()
    unique_news = []
    for news in validated_news:
        if not news.url or news.url in seen_urls:
            continue
        seen_urls.add(news.url)
        unique_news.append(news)

    logger.info(f"–ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(unique_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

    # =========================
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    # =========================
    keywords = settings.keywords_list
    if keywords:
        filtered_news = []
        for news in unique_news:
            text = f"{news.title} {news.summary}".lower()
            if any(word in text for word in keywords):
                filtered_news.append(news)
        logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
    else:
        filtered_news = unique_news

    # =========================
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ
    # =========================
    filtered_news.sort(
        key=lambda x: x.published_at or "",
        reverse=True
    )

    return filtered_news


# =========================
# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
# =========================
async def main():
    news = await collect_news(limit_telegram=20)
    for idx, item in enumerate(news, 1):
        print(f"{idx}. [{item.source} | {item.source_type}] {item.title} ({item.published_at})")
        print(f"   {item.url}\n")
        print(f"–ö—Ä–∞—Ç–∫–æ: {item.summary}\n")


if __name__ == "__main__":
    asyncio.run(main())
