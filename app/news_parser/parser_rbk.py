# app/news_parser/parser_rbk.py
from bs4 import BeautifulSoup
from typing import List, Dict
from datetime import datetime

from app.logger import logger
from app.news_parser.load_site import fetch_html
from app.config import settings
from app.utils.rate_limit import random_delay


async def parse_news_rbk_site(url: str = None, source_name: str = "rbc.ru") -> List[Dict]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å RBC. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–∫–∞–∑–∞–Ω–∏–µ URL –∏ source_name.

    :param url: URL —Å–∞–π—Ç–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞. –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫.
    :param source_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è source)
    :return: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ ParsedNewsSchema
    """
    url = url or settings.rbc_url
    html = await fetch_html(url)

    if not html:
        logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π HTML –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}")
        return []

    logger.info(f"üåê –ü–æ–ª—É—á–µ–Ω HTML –∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}")

    soup = BeautifulSoup(html, "html.parser")
    news_items: List[Dict] = []

    main_content = soup.select_one(".l-col-main")
    if not main_content:
        logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä {source_name}")
        return []

    articles = main_content.find_all("div", class_="item__wrap l-col-center")
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π {source_name}: {len(articles)}")

    for item in articles:
        await random_delay(2.0, 5.0)

        try:
            title_tag = item.find("a", class_="item__link")
            if not title_tag:
                continue

            title = title_tag.get_text(strip=True)[:300]
            href = title_tag.get("href")
            if not title or not href or not href.startswith("http"):
                continue

            html_article = await fetch_html(href)
            if not html_article:
                continue

            article_soup = BeautifulSoup(html_article, "html.parser")
            content_tag = article_soup.find("div", class_="l-col-center-590 article__content")
            if not content_tag:
                continue

            text_block = content_tag.find("div", class_="article__text")
            if not text_block:
                continue

            full_text = text_block.get_text(strip=True)
            if not full_text or len(full_text) < 50:
                continue

            summary = full_text[:400] + "..." if len(full_text) > 400 else full_text

            # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            published_at = None
            time_tag = content_tag.find("time")
            if time_tag and time_tag.get("datetime"):
                try:
                    published_at = datetime.fromisoformat(time_tag.get("datetime").replace("Z", "+00:00"))
                except ValueError:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É {source_name}: {time_tag.get('datetime')}")

            news_items.append(
                {
                    "title": title,
                    "url": href,
                    "summary": summary,
                    "published_at": published_at,
                    "raw_text": full_text,
                    "source": source_name,
                    "source_type": "site",
                    "source_url": url,
                }
            )

        except Exception:
            logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —Å—Ç–∞—Ç—å–∏ {source_name}")
            continue

    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π {source_name}: {len(news_items)}")
    return news_items

